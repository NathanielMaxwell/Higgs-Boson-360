---
title: "A Bayesian Analysis of Spotify Data "
author: "Nathaniel Maxwell, Jessie Bierschenk"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, message=F, warning=F, echo=F}
#
require(tidyverse)
require(rstanarm)
require(magrittr)
library(tidyverse)
library(ggplot2)
require(loo)
require(bayesplot)
require(caret)
library(leaps)
library(rstan)
require(HSAUR3)
library(dplyr)
library(gridExtra)
library(corrplot)
library(projpred)
library(GGally)
library(tidyverse)
library(tidyverse)
library(broom)
library(knitr)
library(patchwork) 

#
ggplot2::theme_set(ggplot2::theme_bw())
knitr::opts_chunk$set(fig.align = 'center')
```

```{r, echo=FALSE, results=FALSE, results='hide', message=FALSE, warning=FALSE}
Logs <- data.frame(read_csv("data/log_sample_reduced.csv"))
Tracks <- data.frame(read_csv("data/tf_sample_1.csv"))
Append <- data.frame(read_csv("data/tf_sample_2.csv"))
```


```{r, echo=FALSE}
Tracks$bounciness <- Append$bounciness
Tracks$danceability <- Append$danceability
Tracks$energy <- Append$energy
Tracks$instrumentalness <- Append$instrumentalness
Tracks$mode <- Append$mode
Tracks$speechiness <- Append$speechiness
Tracks$tempo <- Append$tempo
Tracks$valence <- Append$valence
```


```{r, echo=FALSE}
Tracks.bool <- Tracks
Tracks.bool$skipped <- rep(1, length(Tracks$track_id))
c <- rep(1,length(Tracks$track_id))
for (i in 1:length(Tracks$track_id)) {
  c[i] <- i
}
vect <- rep(1,17468)
for (i in 33236:50704) {
  vect[i-33235] <- i
}
Leftover <- Tracks.bool[-vect,]
Tracks.final <- rbind(Tracks.bool, Leftover)
```

```{r, Combining Data, cache = TRUE, echo=FALSE}
for (i in 1:length(Logs$track_id_clean)) {
  x <- Logs$track_id_clean[[i]]
  y <- which(Tracks$track_id == x)
  bool <- 1
  if (Logs$not_skipped[[i]] == TRUE) {
    bool <- 0
  }
  z <- cbind(Tracks[y,], skipped = bool)
  Tracks.final[i,] <- z
}
```

```{r, echo=FALSE}
Tracks.final$skipped <- as.factor(Tracks.final$skipped)
Track_features <- Tracks.final[Tracks.final$release_year >= 2010,]
Track_features <- Track_features[Track_features$speechiness <= 0.4,]
Track_features <- Track_features[Track_features$instrumentalness <= 0.6,]
Track_features <- Track_features[Track_features$duration <= 360,]
# scale the covariates for easier comparison of coefficient posteriors
Track_features$us_popularity_estimate <- scale(Track_features$us_popularity_estimate)
Track_features$duration <- scale(Track_features$duration)
Track_features$acousticness <- scale(Track_features$acousticness)
Track_features$beat_strength <- scale(Track_features$beat_strength)
Track_features$bounciness <- scale(Track_features$bounciness)
Track_features$danceability <- scale(Track_features$danceability)
Track_features$energy <- scale(Track_features$energy)
Track_features$instrumentalness <- scale(Track_features$instrumentalness)
Track_features$speechiness <- scale(Track_features$speechiness)
Track_features$tempo <- scale(Track_features$tempo)
Track_features$valence <- scale(Track_features$valence)
```

\section{Introduction}
  Music-making is often thought of as an artform—a subjective expression that falls into a specific “genre” according to its musical attributes. Beginning in the 1960s, pop music had been dominated by the verse-chorus form where “the verse sets the scene, the pre-chorus builds tension, and the chorus reaches a climax,” with the cycle predictably repeating itself [5]. This musical formula dominated the industry; in fact, “music theorist Jay Summach has found that by the end of the 1960s, 42 percent of hit songs used verse-chorus form. By the end of the 1980s, that figure had doubled to 84 percent” [5].
    
  With the advent of the 21st Century, however, the digitization of music production paired with the introduction of streaming platforms has warped the fundamental structure of songs. On popular media platforms, only snapshots of songs reach the ears of the public: five-second memes, 15-second TikToks, or 30-second ads. The limitless access to songs on streaming platforms has changed the landscape of song-making—"the gist of it: songwriters now get to the good stuff sooner” [2]. This phenomenon exists as increasing accessibility of songs results in decreasing revenue for artists. “Artists are paid per play—provided the listener stays tuned for at least 30 seconds. Each stream earns a tiny fraction of a cent. And just 13% of that goes to the songwriter, says David Israelite of the National Music Publishers Association” [Economist]. In turn, for an artist to make a decent living, their songs need millions of plays.
   
  For many musicians, the art of composing/performing/marketing a new song is an arduous process. Even after all the work has been completed and a song is ready to be played to the public, the biggest uncertainty still awaits: How will the song be received? Will it become a hit? Will it be a song that everyone skips over, or never becomes popular? The purpose of this analysis is to investigate which characteristics of a song (such as tempo, duration, mode, acousticness, etc.) would make it more “likeable,” less likely to be skipped, or more popular. Of course, music taste is a very subjective matter, and thus, there will be quite a bit of uncertainty around any variables that are deemed important/unimportant. What one person likes; another person may dislike. Therefore, looking at such musical characteristics through a Bayesian lens will help to quantify the uncertainty surrounding any of our findings. Through this analysis we hope to provide some conclusions that an aspiring musician (or even a well-established musician) can have at their disposal when creating new music.
   
  These findings beg the question: so what are the features that make a song popular or appealing to a listener? Answers that would be valuable for any musician seeking success in today’s music industry. 

\section{Pre-Analysis}
\subsection{Data}

$\textbf{Data set 1:}$ In our initial data set, we take a sample of 89,393 tracks from 167,881 released by Spotify that document musical attributes of the track, as well as if it was skipped by a listener on Spotify or not. The set of 167,881 observations was a sample of a full set of over 30 million observations. The original purpose of the full data set was to analyze track attributes in order to predict whether a track would be skipped by a listener in the future. The tracks within the data set were not confined to any prerequisite of genre or form; perhaps some were not even songs, but rather audiobooks or podcasts. We wanted to attempt to narrow the track selection to represent only songs, and see if any variables could impact the "popularity" of a track. The explanation of narrowing process is explained in detail in the variable explanation section
  
  Because this data set was not confined to an individual “taste” nor genre, limitations of this data were that song features may not be determined as significant due to the broadness of the data collected. Furthermore, there was no genre assigned to each data point, so it was impossible to narrow the data to be more specific to a particular sub-industry. Because of this, we recognized that in order to create a more meaningful report, we would have to narrow our focus and our data.  
  
$\textbf{Data set 2:}$ With the limitation of the first data set, we decided to narrow our focus and analyze data that pertained to one specific individual that recorded his tastes for 2,000 songs. For this second dataset, the size was not as big because it presented the opinions of a single individual. George McIntire assembled this data when exploring the explanation behind his varying taste in music. He created two playlists—each with 1,000 songs—one with songs he liked and the other with songs he did not like [4]. In order to minimize bias, he had to make the “BAD” playlist equally as diverse as the “GOOD” playlist by putting in songs of different form rather than an entire album of a single artist he did not like.

Before beginning our analysis of the data, we removed the name and the artist of each song to avoid an unnecessary amount of dummy variables. Furthermore, as a new musician cannot attempt to be a different artist, that information is not important to our analysis. However, when observing the original data, it was clear that popular artists such as Drake and Young Thug frequented list. We also decided to exclude the variable key, since only about 1 in 10,000 people have perfect pitch, and thus would be able to identify the key [3].

Limitations of this data can be attributed to the source being a single individual as well as the possibility that “taste” in music can not be fully associated with the variables in the dataset.



$\textbf{Data set 1 Importation Method:}$ The initial dataset came in two files. The first file contained the track id and whether or not the track was skipped (186,000 observations), and the second file contained the attributes of the tracks (of which there were 50,704 distinct tracks). Given that Rstudio only takes file sizes of less than 5MB, we extracted every other observation to cut the first file size in half to 83,939. For the second file, we discarded variables that would have no interpretation, and given that the file was still above 5MB, we split it into half, and imported both of them, along with the first file, into Rstudio. Finally, we combined the three files all into 1 file to obtain 83,939 observations containing track attributes and whether or not that track was skipped.
 
$\textbf{Data set 2 Importation Method:}$ 
 
$\textbf{Data set Descriptors:}$ 
Here are the descriptors of variables for the two datasets.

\begin{enumerate}
    \item The first dataset consists of 83,939 observations on Spotify of whether or not a track was skipped by users. In total, 65,417 different tracks were included in the dataset. The response variable is “skipped” (1 if skipped and 0 if not), and is dependent upon the variables containing musical attributes, which will be described below. Each track has the following characteristics:
    \begin{enumerate}
        \item Release Year (Year the song was released)
        \item Duration (length of song in seconds)
        \item US Popularity Estimate (A popularity rating of song, on a scale 1-100)
        \item Acousticness (A confidence measure from 0-1 on whether the track is acoustic, where values near 1 represent high confidence that the track is acoustic)
        \item Beat Strength (The strength of the beat from 0-1, where 1 represents a very strong sense of beat)
        \item Bounciness (A rating of the bounciness from 0-1, where 1 represents a strong sense of bounciness)
        \item Danceability (A rating from 0-1 of how suitable the track is for dancing, where values near 1 represent high suitability)
        \item Energy (A rating from 0-1 representing a perceptual measure of intensity and activity, where values near 1 represent high energy)
        \item Instrumentalness (A rating from 0-1 that predicts whether a track has no vocals, where values close to 1 represent high confidence that there are no vocals)
        \item Mode (Predicts whether or not a song is major or minor)
        \item Speechiness (A rating from 0-1 that detects the presence of spoken words in a track, with values near 1 representing an exclusively speech-like track)
        \item Tempo (The estimated tempo of the track in Beats Per Minute (BPM))
        \item Valence (A rating from 0-1 that represents the positivity of the song, with 1 representing high positivity)
        \item Skipped (Denotes whether or not that particular track was skipped or played the entire way through)
    \end{enumerate}
    \textbf{Note}: In order to try to obtain tracks most representative of new music, only the following tracks were kept (total of 65,417 observations kept):
    \begin{enumerate}
        \item Tracks from 2010-present
        \item Tracks with a speechiness value <= 0.4 (filters out tracks that are mostly spoken, such as podcasts and ebooks)
        \item Tracks with an instrumentalness value <= 0.6 (filters out tracks that contain no vocals)
        \item Tracks with a duration <= 360 seconds (given that the average new song is 3-5 minutes, a cutoff of 6 minutes seemed appropriate)
        Furthermore, the variables year and track id were not utilized in the analysis, as their information provides no benefit to a songwriter.
    \end{enumerate}
    \item The second dataset consisted of 2017 songs compiled by a single person, where a portion of the songs are songs that he likes, and the other portion are songs that he dislikes. This dataset includes similar variables as the first dataset, including:
    \begin{enumerate}
        \item Acousticness
        \item Danceability 
        \item Duration
        \item Energy
        \item Instrumentalness
        \item Key (The particular grouping of chords and notes in a song) 
        \item Liveness (rating from 0-1 of whether the track was performed live, with 1 representing high confidence the track was performed live)
        \item Loudness (Overall loudness of the track in decibles (dB))
        \item Mode
        \item Speechiness
        \item Tempo
        \item Time Signature (The way in which beats of the song are organized)
        \item Valence
    \end{enumerate}
\end{enumerate}



\subsection{Model Selection}
For the first dataset, our response variable, $\textbf{y}$, will be modeled by a $Bernoulli(\theta)$ distribution, where 1 means the track was skipped, and 0 means the track was not skipped. To obtain the variable $\theta$, we will use the logit link, where $logit(\theta) = \eta$, and $\eta = \textbf{x}^T\boldsymbol{\beta}$, where $\textbf{x} \in \textbf{X}$ is the covariate space for $\textbf{Y}$. In other words, we are creating a logistic regression model where $\theta$ is obtained from a linear model. We can write our sampling distribution for$\textbf{y}$ as $[\textbf{y}|\theta]$, and since $\theta$ is dependent upon $\boldsymbol{\beta}$ and $\textbf{x}$, we can write this as $[\textbf{y}|\boldsymbol{\beta},\textbf{x} ]$. 

What we really want is to estimate the values of the coefficients $\boldsymbol{\beta}$ for each of the variables to find out how they impact whether or not a track is skipped. In order to form a posterior estimate for $\boldsymbol{\beta}$, we must specify a prior. We assume little knowledge about each variable's effect, so we propose a weakly informative prior for $[\boldsymbol{\beta}]$: Using recommendations from Gelman, Jakulin, Pittau, and Su [7], we use a Cauchy(0,2.5) prior for each variable. Furthermore, we centered all the variables and then scaled them to have the same standard deviation, so that no variable could have a disproportionate effect on the outcome, and thus have better interpretability. Now that we have a prior model $[\boldsymbol{\beta}]$ and sampling model $[\textbf{y}|\theta]$,we can use all $\textbf{y}_i \in \textbf{Y}$ to calculate the posterior $[\boldsymbol{\beta}|\textbf{Y}, \textbf{X}]$ using Baye's Theorem and proportionality. This will not be done by hand. Instead, using the rstanarm package, Rstudio will compute the posterior and draw MCMC samples from the $[\boldsymbol{\beta}|\textbf{Y}, \textbf{X}]$. In this way we will obtain estimates for the values of $\beta_0, \beta_1,...,\beta_k \in \boldsymbol{\beta}$.

Note: In order to allow efficient posterior sampling, a random sample of 5000 tracks were used.



```{r, echo=FALSE, fig.show='hide', message=FALSE}
p1= ggplot(data = Track_features, aes(x = duration)) +
  geom_histogram()
p2= ggplot(data = Track_features, aes(x = us_popularity_estimate)) +
  geom_histogram()
p3= ggplot(data = Track_features, aes(x = acousticness)) +
  geom_histogram()
p4= ggplot(data = Track_features, aes(x = beat_strength)) +
  geom_histogram()
p5= ggplot(data = Track_features, aes(x = bounciness)) +
  geom_histogram()
p6=ggplot(data = Track_features, aes(x = danceability)) +
  geom_histogram()
p7= ggplot(data = Track_features, aes(x = energy)) +
  geom_histogram()
p8=ggplot(data = Track_features, aes(x = instrumentalness)) +
  geom_histogram()
p9=ggplot(data = Track_features, aes(x = mode)) +
  geom_bar()
p10=ggplot(data = Track_features, aes(x = speechiness)) +
  geom_histogram()
p11= ggplot(data = Track_features, aes(x = tempo)) +
  geom_histogram()
p12=ggplot(data = Track_features, aes(x = valence)) +
  geom_histogram()
p13=ggplot(data = Track_features, aes(x = skipped)) +
  geom_bar()

grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10,p11, p12, p13, nrow=2)
```

```{r, echo=FALSE, fig.show='hide', message=FALSE}
g1= ggplot(Track_features, aes(skipped, duration)) + geom_boxplot()
g2=ggplot(Track_features, aes(skipped, us_popularity_estimate)) + geom_boxplot()
g3=ggplot(Track_features, aes(skipped, acousticness)) + geom_boxplot()
g4=ggplot(Track_features, aes(skipped, beat_strength)) + geom_boxplot()
g5=ggplot(Track_features, aes(skipped, bounciness)) + geom_boxplot()
g6=ggplot(Track_features, aes(skipped, danceability)) + geom_boxplot()
g7=ggplot(Track_features, aes(skipped, energy)) + geom_boxplot()
g8=ggplot(Track_features, aes(skipped, instrumentalness)) + geom_boxplot()
g9=ggplot(Track_features, aes(skipped, speechiness)) + geom_boxplot()
g10=ggplot(Track_features, aes(skipped, tempo)) + geom_boxplot()
g11=ggplot(Track_features, aes(skipped, valence)) + geom_boxplot()

grid.arrange(g1, g2, g3, g4, g5, g6, g7, g8, g9, g10,g11,  nrow=2)
```


\section{Posterior Estimates}
```{r, echo=FALSE}
set.seed(2)
a <- sample.int(length(Track_features$track_id), 5000)
Track_features_a <- Track_features[a,]
Track_features_a <- Track_features_a[2:15]
Track_features_a <- Track_features_a[-c(2)] 
```


```{r, echo=FALSE, cache=TRUE}
seed <- 1
posterior1 <- stan_glm(skipped ~ ., data = Track_features_a,
                 family = binomial(link = "logit"), 
                 prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                 seed = seed,
                 refresh = 0)
```

```{r, echo=FALSE, cache=TRUE}
POST1 <- mcmc_areas(as.matrix(posterior1), prob = 0.95, prob_outer = 1)
POST1
round(posterior_interval(posterior1, prob = 0.95), 3)
```

```{r, echo=FALSE, message=FALSE, results='hide', cache=TRUE}
(loo1 <- loo(posterior1, save_psis = TRUE))
```

```{r,  echo=FALSE, message=FALSE, results='hide', cache=TRUE}
posterior0 <- stan_glm(skipped ~ 1, data = Track_features_a,
                 family = binomial(link = "logit"), 
                 prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                 seed = seed,
                 refresh = 0)
(loo0 <- loo(posterior0, save_psis = T))
rstanarm::loo_compare(loo0, loo1)
```
After running the rstanarm function and including all of the variables, we see that there is only variable whose 95% confidence interval does not include 0. That variable is duration, and furthermore, when calculating the 'leave-one-out' cross-validation information criterion (looic), we see that this model barely has a slightly lower value (6366) than the looic of a baseline model (6373) with no predictors.  To attempt to find the best model, we will drop all variables that were not deemed significant at a 95% confidence interval (included 0 in their posterior interval), and rerun the model. In this case, 'duration' is the only variable remaining.

```{r, echo=FALSE, results='hide', cache=TRUE}
posterior2 <- stan_glm(skipped ~ duration, data = Track_features_a,
                 family = binomial(link = "logit"), 
                 prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                 seed = seed,
                 refresh = 0)
POST2 <- mcmc_areas(as.matrix(posterior2), prob = 0.95, prob_outer = 1)
round(posterior_interval(posterior2, prob = 0.95), 3)
```
```{r,results='hide', message=FALSE, cache=TRUE}
(loo2 <- loo(posterior2, save_psis = TRUE))
rstanarm::loo_compare(loo0, loo2)
```


```{r, message=FALSE, fig.show='hide', results='hide', cache=TRUE}
matrix2<-as.matrix(posterior2)
trace2 <- plot(matrix2[,2], type="l")
ACF2 <- acf(matrix2[,2])
```

This model proved to be better, with a looic value of 6358. To ensure that the MCMC samples created from the stan_glm fuction converged and mixed well, a traceplot and ACF plot confirmed that there were no problems (See appendix for graphs). To calculate our posterior predictive accuracy, we used the following method. If the posterior probability of a track being skipped is greater or equal to $0.5$, then we would predict that observation to have a value of 1 (and similarly for less than $0.5$). For each observation, we can compare the posterior prediction to the actual observed value. The proportion of times we correctly predict an individual (i.e. [prediction = 0 and observation = 0] or [prediction = 1 and observation = 1]) is our classification accuracy. In our case, the posterior classification accuracy is 0.65. While we would really want to also calculate the estimated accuracy on "unseen" data, or data that doesn't actually affect this model, because our number of observations is so large, we would expect the value to be the same when using a LOOCV, and this is indeed true: the value is still 0.65.

```{r, echo=FALSE, message=FALSE, cache=TRUE}
preds2 <- posterior_linpred(posterior2, transform = TRUE)
pred2 <- colMeans(preds2)
```

```{r, echo=FALSE, message=FALSE, results='hide', cache=TRUE}
pr2 <- as.integer(pred2 >= 0.5)
round(mean(xor(pr2,as.integer(Track_features_a$skipped == 0))),3)
```

```{r, echo=FALSE, message=FALSE, results='hide', cache=TRUE}
ploo2 = E_loo(preds2, loo2$psis_object, type="mean", log_ratios = -log_lik(posterior2))$value
round(mean(xor(ploo2 > 0.5,as.integer(Track_features_a$skipped==0))),3)
```
\section{Conclusion}
Our analysis of this dataset turned out to be rather inconclusive. It only returned one variable as significant, and that was duration. In effect, it stated that the longer the track is, the more likely it is to be skipped. Other than that, there were no other findings. This lack of conclusive analysis could be explained in a variety of ways. One, we do not have any guarantee that the tracks analyzed were actually songs. While we applied filters to attempt to filter out any tracks that were not songs, we still do not actually know the content of the track. So the uncertainty surrounding the track content is one factor that is certainly a model limitation. A second reason behind inconclusiveness is the fact that music taste is a very subjective field, and what one person likes, another person may not. Given that this data is pulled from a multitude of Spotify users, there is going to be a wide range of musical preferences. Therefore, it would be much harder to find any nuances in music style that might affect the broader population’s tendency to skip or not skip a song. An interesting note is that the duration of a song, our only significant variable, has been decreasing when looking specifically at the #1 song by year in the last few years, according to the New York Times [5]. This overall trend for all songs is backed by an article from the Economist that echoes that same sentiment [2]. It is clear, then, that this trend is evident in a Spotify user’s likelihood to skip or not skip a song.


###New Data
```{r, echo=FALSE, cache = TRUE, message=FALSE, warning=FALSE}
spotify <- data.frame(read_csv("data/spotify.csv"))
#View(spotify)
```

```{r, echo=FALSE}
#Drop un-needed variables
spotify1 <- spotify[-c(1,7,16,17)]

spotify1$target <- factor(spotify1$target)
spotify1$mode <- factor(spotify1$mode)
spotify1$time_signature <- factor(spotify1$time_signature)

spotify1 <- spotify1 %>% 
   mutate(duration_ms = duration_ms / 1000)
# scale the covariates for easier comparison of coefficient posteriors
spotify1$acousticness <- scale(spotify1$acousticness)
spotify1$danceability <- scale(spotify1$danceability)
spotify1$duration_ms <- scale(spotify1$duration_ms)
spotify1$energy <- scale(spotify1$energy)
spotify1$instrumentalness <- scale(spotify1$instrumentalness)
spotify1$liveness <- scale(spotify1$liveness)
spotify1$loudness <- scale(spotify1$loudness)
spotify1$speechiness <- scale(spotify1$speechiness)
spotify1$tempo <- scale(spotify1$tempo)
spotify1$valence <- scale(spotify1$valence)
```

```{r, echo=FALSE, cache=TRUE, message=FALSE}
#EDA
a1= ggplot(data = spotify1, aes(x = duration_ms)) +
  geom_histogram()
a2= ggplot(data = spotify1, aes(x = instrumentalness)) +
  geom_histogram()
a3= ggplot(data = spotify1, aes(x = liveness)) +
  geom_histogram()
a4= ggplot(data = spotify1, aes(x = loudness)) +
  geom_histogram()
a5= ggplot(data = spotify1, aes(x = speechiness)) +
  geom_histogram()
a6=ggplot(data = spotify1, aes(x = tempo)) +
  geom_histogram()
a7= ggplot(data = spotify1, aes(x = valence)) +
  geom_histogram()
a8=ggplot(data = spotify1, aes(x = acousticness)) +
  geom_histogram()
a9=ggplot(data = spotify1, aes(x = danceability)) +
  geom_histogram()
a9=ggplot(data = spotify1, aes(x = energy)) +
  geom_histogram()
a10=ggplot(data = spotify1, aes(x = target)) +
  geom_bar()
grid.arrange(a1, a2, a3, a4, a5, a6, a7, a8, a9, a10,  nrow=2)
```


```{r, echo=FALSE}
spotify2 <- spotify1
#spotify2$key <- as.numeric(spotify2$key)
spotify2$target <- as.numeric(spotify2$target)
  #this makes target 2 or 1
spotify2$mode <- as.numeric(spotify2$mode)
spotify2$time_signature <- as.numeric(spotify2$time_signature)
```


```{r, echo=FALSE}
corrplot(cor(spotify2))
```


```{r, cache = TRUE, echo = FALSE, message=FALSE}
seed=1
posterior3 <- stan_glm(target ~ ., data = spotify1,
                 family = binomial(link = "logit"), 
                 prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                 seed = seed,
                 refresh = 0)

```


```{r, cache = T, echo=FALSE}
POST3 <- mcmc_areas(as.matrix(posterior3), prob = 0.90, prob_outer = 1)
round(coef(posterior3), 3)
round(posterior_interval(posterior3, prob = 0.90), 3)
```


```{r, cache = T}
(loo3 <- loo(posterior3, save_psis = TRUE))
```

#Model Selection


```{r, cache = T, echo=FALSE}
posterior4 <- stan_glm(target ~ 1, data = spotify1,
                 family = binomial(link = "logit"), 
                 prior = normal(0,1), prior_intercept = normal(0,1),
                 seed = seed,
                 refresh = 0)
(loo4 <- loo(posterior4, save_psis = T))
rstanarm::loo_compare(loo3, loo4)
```

```{r, cache=TRUE}
posterior4.1 <- stan_glm(target ~ danceability+ duration_ms+ energy+ instrumentalness+ liveness+ loudness+ mode+ speechiness+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior4.2 <- stan_glm(target ~ acousticness+ duration_ms+ energy+ instrumentalness+ liveness+ loudness+ mode+ speechiness+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior4.3 <- stan_glm(target ~ acousticness+ danceability+ energy+ instrumentalness+ liveness+ loudness+ mode+ speechiness+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior4.4 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+ instrumentalness+ liveness+ loudness+ mode+ speechiness+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior4.5 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+ energy+ liveness+ loudness+ mode+ speechiness+ tempo+  valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior4.6 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+ energy+ instrumentalness+ loudness+ mode+ speechiness+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior4.7 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+ energy+ instrumentalness+ liveness+ mode+ speechiness+ tempo+  valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior4.8 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+ energy+ instrumentalness+ liveness+ loudness+ speechiness+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior4.9 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+ energy+ instrumentalness+ liveness+ loudness+ mode+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior4.10 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+ energy+ instrumentalness+ liveness+ loudness+ mode+ speechiness+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior4.11 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+ energy+ instrumentalness+ liveness+ loudness+ mode+ speechiness+ tempo, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior4.full <- stan_glm(target ~ acousticness+ danceability+ duration_ms+ energy+ instrumentalness+ liveness+ loudness+ mode+ speechiness+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
```

```{r, cache=TRUE, message = FALSE, results='hide'}
(loo4.1 <- loo(posterior4.1, save_psis = T))
(loo4.2 <- loo(posterior4.2, save_psis = T))
(loo4.3 <- loo(posterior4.3, save_psis = T))
(loo4.4 <- loo(posterior4.4, save_psis = T))
(loo4.5 <- loo(posterior4.5, save_psis = T))
(loo4.6 <- loo(posterior4.6, save_psis = T))
(loo4.7 <- loo(posterior4.7, save_psis = T))
(loo4.8 <- loo(posterior4.8, save_psis = T))
(loo4.9 <- loo(posterior4.9, save_psis = T))
(loo4.10 <- loo(posterior4.10, save_psis = T))
(loo4.11 <- loo(posterior4.11, save_psis = T))
(loo4.full <- loo(posterior4.full, save_psis = T))
```

```{r, cache=TRUE, results='hide'}
rstanarm::loo_compare(loo4.1, loo4.2, loo4.3, loo4.4, loo4.5, loo4.6, loo4.7, loo4.8, loo4.9, loo4.10, loo4.11, loo4.full)
```


```{r, cache=TRUE}
posterior5.1 <- stan_glm(target ~ danceability+ duration_ms+ instrumentalness+ liveness+ loudness+ mode+ speechiness+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior5.2 <- stan_glm(target ~ acousticness+ duration_ms+  instrumentalness+ liveness+ loudness+ mode+ speechiness+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior5.3 <- stan_glm(target ~ acousticness+ danceability+  instrumentalness+ liveness+ loudness+ mode+ speechiness+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)

posterior5.4 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+ liveness+ loudness+ mode+ speechiness+ tempo+  valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior5.5 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+ instrumentalness+ loudness+ mode+ speechiness+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior5.6 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+  instrumentalness+ liveness+mode+ speechiness+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior5.7 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+  instrumentalness+ liveness+ loudness+ speechiness+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior5.8 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+  instrumentalness+ liveness+ loudness+ mode+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior5.9 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+  instrumentalness+ liveness+ loudness+ mode+ speechiness+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior5.10 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+  instrumentalness+ liveness+ loudness+ mode+ speechiness+ tempo, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
posterior5.full <- stan_glm(target ~ acousticness+ danceability+ duration_ms+ instrumentalness+ liveness+ loudness+ mode+ speechiness+ tempo+ valence, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)
```

```{r, cache=TRUE, message = FALSE, results='hide'}
(loo5.1 <- loo(posterior5.1, save_psis = T))
(loo5.2 <- loo(posterior5.2, save_psis = T))
(loo5.3 <- loo(posterior5.3, save_psis = T))
(loo5.4 <- loo(posterior5.4, save_psis = T))
(loo5.5 <- loo(posterior5.5, save_psis = T))
(loo5.6 <- loo(posterior5.6, save_psis = T))
(loo5.7 <- loo(posterior5.7, save_psis = T))
(loo5.8 <- loo(posterior5.8, save_psis = T))
(loo5.9 <- loo(posterior5.9, save_psis = T))
(loo5.10 <- loo(posterior5.10, save_psis = T))
(loo5.full <- loo(posterior5.full, save_psis = T))
```

```{r, cache=TRUE, results='hide'}
rstanarm::loo_compare(loo5.1, loo5.2, loo5.3, loo5.4, loo5.5, loo5.6, loo5.7, loo5.8, loo5.9, loo5.10, loo5.full)
```

```{r, cache=TRUE, results='hide'}
posterior5.interaction <- stan_glm(target ~ acousticness+ danceability+ duration_ms+ instrumentalness+ liveness+ loudness+ mode+ speechiness+ tempo+ valence + valence*mode+ duration_ms*danceability+ acousticness*liveness, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)

(loo5.interaction <- loo(posterior5.interaction, save_psis = T))
```

```{r, cache=TRUE,results='hide'}
rstanarm::loo_compare(loo5.interaction, loo5.full)
```


```{r, cache=TRUE, results='hide'}
posterior5.interaction2 <- stan_glm(target ~ acousticness+ danceability+ duration_ms+ instrumentalness+ liveness+ loudness+ mode+ speechiness+ tempo+ valence + duration_ms*danceability+ acousticness*liveness, data = spotify1,
                family = binomial(link = "logit"), 
                prior = cauchy(0,2.5), prior_intercept = cauchy(0,2.5),
                seed = seed,
                refresh = 0)

(loo5.interaction2 <- loo(posterior5.interaction2, save_psis = T))
```

```{r, cache=TRUE, results='hide'}
rstanarm::loo_compare(loo5.interaction, loo5.interaction2)
```

```{r, cache = T, echo=FALSE}
POST5 <- mcmc_areas(as.matrix(posterior5.interaction2), prob = 0.90, prob_outer = 1)
round(coef(posterior5.interaction2), 3)
round(posterior_interval(posterior5.interaction2, prob = 0.90), 3)
```
To calculate our posterior predictive accuracy, we used the following method. If the posterior probability of the song being liked for an particular song is greater or equal to $0.5$, then we would predict that that song to have a value of 1 (and similarly for less than $0.5$). For each observation, we can compare the posterior prediction to the actual observed value. The proportion of times we correctly predict an individual (i.e. [prediction = 0 and observation = 0] or [prediction = 1 and observation = 1]) is our classification accuracy.


```{r, echo=FALSE, message=FALSE}
preds5 <- posterior_linpred(posterior5.interaction2, transform = TRUE)
pred5 <- colMeans(preds5)
```

```{r, echo=FALSE, cache = TRUE}
pr5 <- as.integer(pred5 >= 0.5)
# have the students calculate this themselves?
round(mean(xor(pr5,as.integer(spotify1$target == 0))),3)
```

```{r, cache = TRUE, echo=FALSE}
ploo5 = E_loo(preds5, loo5.interaction2$psis_object, type="mean", log_ratios = -log_lik(posterior5.interaction2))$value
round(mean(xor(ploo5>0.5,as.integer(spotify1$target==0))),3)
```

```{r, cache = TRUE, echo=FALSE}
#acousticness
matrix5 <- as.matrix(posterior5.interaction2)
trace5.acoust <- plot(matrix5[,2], type="l")
ACF5.acoust <- acf(matrix5[,2])

#danceability
trace5.dance <- plot(matrix5[,3], type="l")
ACF5.dance <- acf(matrix5[,3])

#duration
trace5.duration <- plot(matrix5[,4], type="l")
ACF5.duration <- acf(matrix5[,4])

#instrumentalness
trace5.instru <- plot(matrix5[,5], type="l")
ACF5.instru <- acf(matrix5[,5])

#liveness
trace5.liveness <- plot(matrix5[,5], type="l")
ACF5.liveness <- acf(matrix5[,5])

#loudness
trace5.loudness <- plot(matrix5[,7], type="l")
ACF5.loudness <- acf(matrix5[,7])

#mode
trace5.mode <- plot(matrix5[,8], type="l")
ACF5.mode <- acf(matrix5[,8])

#speechness
trace5.speech <- plot(matrix5[,9], type="l")
ACF5.speech <- acf(matrix5[,9])

#tempo
trace5.tempo <- plot(matrix5[,10], type="l")
ACF5.tempo <- acf(matrix5[,10])

#valence
trace5.valence <- plot(matrix5[,11], type="l")
ACF5.valence <- acf(matrix5[,11])

#interaction
trace5.duration.dance <- plot(matrix5[,12], type="l")
ACF5.duration.dance <- acf(matrix5[,12])

#interaction2
trace5.acoust.live <- plot(matrix5[,13], type="l")
trace5.acoust.live <- acf(matrix5[,13])

```

[1] Betancourt, Michael. How the Shape of a Weakly Informative Prior Affects Inferences,                        mc-stan.org/users/documentation/case-studies/weakly_informative_shapes.html. 

[2] “The Economics of Streaming Is Changing Pop Songs.” The Economist, The Economist Newspaper, 5 Oct. 2019, www.economist.com/finance-and-economics/2019/10/05/the-economics-of-streaming-is-changing-pop-songs. 

[3] Gander, Kashmira. “Perfect Pitch: Why Some People Might Have Rare Musical Skill Possessed by Bach and Mozart.” Newsweek, Newsweek, 22 Feb. 2019, www.newsweek.com/perfect-pitch-why-rare-musical-skill-bach-mozart-1326380.

[4] McIntire, George. “A Machine Learning Deep Dive into My Spotify Data.” Open Data Science - Your News Source for AI, Machine Learning &amp; More, 5 Apr. 2018, opendatascience.com/a-machine-learning-deep-dive-into-my-spotify-data/. 

[5] Sloan, Nate, and Charlie Harding. “The Culture Warped Pop, for Good.” The New York Times, The New York Times, 14 Mar. 2021, www.nytimes.com/interactive/2021/03/14/opinion/pop-music-songwriting.html?auth=login-google1tap&amp;login=google1tap. 

[6] Vehtari, Aki, et al. “Bayesian Logistic Regression with Rstanarm.” Github, 4 Dec. 2019, avehtari.github.io/modelselection/diabetes.html. 

[7]

